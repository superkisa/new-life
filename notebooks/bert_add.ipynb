{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from mujoco import viewer\n",
    "from stable_baselines3 import SAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"Ant-v4\", ctrl_cost_weight=0.1, use_contact_forces=True, render_mode=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:185: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.normal import Normal\n",
    "from transformers import DistilBertConfig, DistilBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorHead(nn.Module):\n",
    "    def __init__(self, config, num_action=2):\n",
    "        super().__init__()\n",
    "        self.dim_action = num_action\n",
    "        self.layer_one = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.num_labels),\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Linear(config.num_labels, self.dim_action * 2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Assuming x is the output from DistilBert\n",
    "        answer = self.layer_one(state)\n",
    "        # answer = self.forward(state)\n",
    "        mu, std = answer[:, : self.dim_action], answer[:, self.dim_action :]\n",
    "        torch.clamp_(std, min=1e-6, max=1)\n",
    "        predicted_gauss = Normal(mu, std)\n",
    "\n",
    "        sample = predicted_gauss.sample()\n",
    "        prob = predicted_gauss.log_prob(sample)\n",
    "        return sample, prob\n",
    "\n",
    "\n",
    "class CustomDistilBertModel(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.custom_head = CustomActorHead(config)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, head_mask=None, inputs_embeds=None\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        # Assuming the last hidden state is used for the head\n",
    "        last_hidden_state = outputs[0]\n",
    "        # Apply your custom head\n",
    "        logits = self.custom_head(last_hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = DistilBertConfig(\n",
    "    vocab_size=10000,\n",
    "    hidden_size=16,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=100,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=50,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "model = CustomDistilBertModel(bert_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# **inputs,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 42\u001b[0m, in \u001b[0;36mCustomDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[0;32m     40\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Apply your custom head\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m, in \u001b[0;36mCustomActorHead.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     16\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m answer[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_action], answer[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_action :]\n\u001b[0;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mclamp_(std, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m predicted_gauss \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m predicted_gauss\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     21\u001b[0m prob \u001b[38;5;241m=\u001b[39m predicted_gauss\u001b[38;5;241m.\u001b[39mlog_prob(sample)\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\distributions\\normal.py:51\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc, scale, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, Number) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scale, Number):\n\u001b[0;32m     53\u001b[0m         batch_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize()\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\distributions\\utils.py:53\u001b[0m, in \u001b[0;36mbroadcast_all\u001b[1;34m(*values)\u001b[0m\n\u001b[0;32m     49\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     50\u001b[0m         v \u001b[38;5;28;01mif\u001b[39;00m is_tensor_like(v) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     51\u001b[0m     ]\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;241m*\u001b[39mnew_values)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    attention_mask=torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    # **inputs,\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2658, 0.1635],\n",
       "         [0.1348, 0.2985],\n",
       "         [0.2441, 0.1565],\n",
       "         [0.2240, 0.1594],\n",
       "         [0.1313, 0.2221]],\n",
       "\n",
       "        [[0.2848, 0.1719],\n",
       "         [0.1523, 0.1742],\n",
       "         [0.2475, 0.1555],\n",
       "         [0.2039, 0.1654],\n",
       "         [0.1115, 0.3331]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomHead, self).__init__()\n",
    "        self.linear = nn.Linear(config.dim, 2 *16)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assuming x is the output from DistilBert\n",
    "        x = self.linear(x)\n",
    "        # return self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomDistilBertModel(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(CustomDistilBertModel, self).__init__(config)\n",
    "        self.custom_head = CustomHead(config)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, head_mask=None, inputs_embeds=None):\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "        # Assuming the last hidden state is used for the head\n",
    "        last_hidden_state = outputs[0]\n",
    "        # Apply your custom head\n",
    "        logits = self.custom_head(last_hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "model = CustomDistilBertModel(bert_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    input_ids=torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    attention_mask=torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    # **inputs,\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.2073e-02, -4.5514e-02,  3.4229e-01,  6.7587e-01,  3.6829e-01,\n",
       "          -5.3154e-01,  9.8220e-02,  7.2635e-01, -1.2696e+00,  4.3517e-01,\n",
       "           8.1557e-01, -9.3974e-01,  2.1697e-01,  6.5389e-01,  2.9961e-01,\n",
       "           3.5622e-01,  2.0894e-01, -7.0037e-01, -5.0135e-01, -1.4976e-01,\n",
       "           1.6277e-01,  5.0670e-01,  8.8279e-01, -5.1181e-01,  8.6238e-01,\n",
       "          -1.1180e+00,  7.6160e-01,  1.8453e-01,  4.6870e-01,  4.4835e-01,\n",
       "          -4.1612e-01, -1.1636e+00],\n",
       "         [-5.0892e-02, -2.8898e-01,  9.4392e-01,  3.0855e-01,  4.2375e-01,\n",
       "           5.3946e-01, -8.1204e-01,  4.4666e-01, -5.9119e-01,  5.8833e-01,\n",
       "           1.1808e-01, -6.2132e-01,  4.9074e-01,  1.0295e+00, -2.3458e-01,\n",
       "           7.0849e-01, -9.1251e-01, -3.7691e-01,  5.5013e-01,  1.1712e-01,\n",
       "           1.4917e-02,  8.2145e-01,  6.3168e-01, -9.0018e-01,  7.5450e-02,\n",
       "          -1.3444e+00,  1.5304e+00,  9.6531e-01,  1.1018e+00, -2.8351e-01,\n",
       "           8.8587e-02, -1.2422e+00],\n",
       "         [ 6.9276e-01, -6.3482e-01,  6.2786e-01,  3.2424e-01,  4.9973e-01,\n",
       "          -1.0756e+00, -1.5855e-01,  9.6888e-01, -9.1279e-01,  2.1780e-01,\n",
       "          -2.3261e-01,  9.3996e-02,  7.7141e-01,  1.0342e+00, -2.7553e-01,\n",
       "           6.2937e-01,  4.2163e-01,  7.9116e-01, -4.5759e-01, -4.5212e-02,\n",
       "          -8.3269e-03, -7.2084e-01,  6.4448e-02, -8.9339e-01,  4.0187e-01,\n",
       "          -1.0426e+00, -3.0476e-01,  3.3951e-01,  1.3943e+00, -1.1964e-01,\n",
       "          -5.6978e-02, -1.2600e+00],\n",
       "         [-3.6276e-01, -8.5384e-01,  5.0812e-01, -1.2142e-01,  1.6627e+00,\n",
       "          -9.9576e-01, -5.7365e-01,  4.8908e-01, -1.1369e+00,  2.9818e-01,\n",
       "           2.7683e-01, -1.6999e-01,  1.0465e+00,  2.8998e-01,  5.9545e-01,\n",
       "           3.4847e-01, -8.1104e-01,  1.5913e-01,  2.6432e-01,  7.1716e-01,\n",
       "           2.3894e-01,  3.5217e-01, -2.3584e-01, -4.2995e-02, -6.9436e-02,\n",
       "          -9.7342e-01,  3.8884e-01, -3.2418e-01,  1.3290e+00,  1.8098e-02,\n",
       "          -6.1826e-01, -8.5703e-01],\n",
       "         [ 1.7556e-01, -1.0657e-01,  2.2510e-01, -1.5352e-01,  9.3670e-01,\n",
       "          -9.2372e-01,  2.5153e-01,  7.1665e-02, -1.1623e+00,  1.8455e-01,\n",
       "           5.7873e-01, -7.7096e-01,  1.4391e+00,  7.0816e-01,  2.4785e-01,\n",
       "           5.9204e-01, -8.7051e-01, -8.6849e-02, -4.1263e-01,  3.2051e-01,\n",
       "           5.3019e-01, -5.1622e-01,  1.4435e-01, -5.8345e-01,  9.0413e-02,\n",
       "          -1.2197e+00,  5.0610e-01,  1.1313e+00,  1.2208e+00, -5.9990e-02,\n",
       "          -6.2284e-01, -1.2955e+00]],\n",
       "\n",
       "        [[-2.8499e-01, -1.8871e-01,  3.4811e-01,  3.6338e-01,  3.7270e-01,\n",
       "          -5.0047e-01, -2.5744e-02,  5.8922e-02, -7.2682e-01,  1.0582e-01,\n",
       "           8.4708e-01, -1.1098e+00,  7.7244e-01,  6.2405e-01, -5.9864e-02,\n",
       "          -5.2864e-02,  3.6189e-01, -3.2774e-01, -3.4363e-01, -8.1574e-02,\n",
       "          -1.1643e-01,  1.7539e-01,  7.5818e-01, -1.0390e+00,  5.7429e-01,\n",
       "          -1.2544e+00,  5.9676e-01,  5.4149e-02,  1.1464e+00,  4.0679e-01,\n",
       "           8.3067e-02, -1.1272e+00],\n",
       "         [-2.4005e-01, -8.3592e-01,  8.5733e-01,  8.9104e-01,  1.1077e-01,\n",
       "          -8.0452e-02, -2.9715e-01,  8.2074e-01, -3.3959e-01,  2.6286e-01,\n",
       "           2.2938e-01, -3.8624e-01,  5.2718e-01,  4.1736e-01, -1.3172e-01,\n",
       "           7.9735e-01,  6.9651e-02, -3.9563e-01,  8.2636e-01,  1.4824e-01,\n",
       "           2.4819e-01,  5.0260e-01, -1.0839e-01, -1.3252e-01,  3.2929e-04,\n",
       "          -1.4244e+00,  1.1177e+00,  3.9937e-01,  1.2212e+00,  1.2717e-01,\n",
       "          -3.4382e-01, -1.1046e+00],\n",
       "         [ 3.5881e-01,  5.1075e-02,  2.9960e-01,  3.6245e-01,  8.8753e-01,\n",
       "          -6.9059e-01, -2.8825e-01,  5.8690e-01, -1.0132e+00,  9.2645e-02,\n",
       "           2.5676e-01, -2.1096e-01, -2.0980e-01,  2.9426e-01,  3.5173e-02,\n",
       "           1.3690e-01,  1.9267e-02,  5.0711e-01, -3.6632e-01, -1.4290e-03,\n",
       "          -1.8767e-01,  1.2870e-01,  4.3430e-01, -6.5263e-01,  2.6287e-01,\n",
       "          -1.0084e+00, -4.8301e-01,  2.7509e-01,  1.0572e+00, -8.4592e-02,\n",
       "           1.3688e-01, -7.0003e-01],\n",
       "         [-3.6565e-01, -8.8783e-01,  1.6982e-01, -3.1529e-01,  1.2946e+00,\n",
       "          -1.2090e+00, -2.1561e-01,  3.1396e-01, -1.0171e+00,  9.8809e-02,\n",
       "           3.3862e-01,  1.0255e-01,  6.0664e-01,  4.7997e-01,  3.5378e-01,\n",
       "           8.8695e-02, -7.7780e-01,  6.6272e-01,  3.6301e-01,  3.9996e-01,\n",
       "          -7.2271e-04,  3.0050e-01, -7.3651e-01, -5.2797e-01, -1.9620e-01,\n",
       "          -7.5602e-01,  5.8980e-01,  1.0588e+00,  1.1205e+00, -1.7353e-01,\n",
       "          -6.0379e-01, -1.1839e+00],\n",
       "         [ 5.8808e-01,  2.5296e-02,  6.3247e-01,  1.1243e-01,  1.7022e+00,\n",
       "          -8.2236e-01,  2.1735e-01,  1.2492e-01, -1.2306e+00, -2.0782e-01,\n",
       "           1.9959e-01, -4.9857e-01,  1.2329e+00,  5.1388e-01, -6.8372e-01,\n",
       "           2.5876e-01, -5.2894e-01,  4.5771e-01, -1.5402e-01,  3.2496e-01,\n",
       "           3.5491e-01, -6.6716e-02, -2.9887e-02, -2.0057e-01,  7.1347e-01,\n",
       "          -1.4490e+00, -3.4931e-01,  1.2189e+00,  1.2941e+00, -5.6740e-01,\n",
       "          -5.7695e-01, -7.0945e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config, action_space):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.fc = nn.Linear(config.dim, 2 * action_space)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, head_mask=None, inputs_embeds=None):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "        # Assuming the last hidden state is used for the head\n",
    "        last_hidden_state = outputs[0]\n",
    "        # Apply the fully connected layer\n",
    "        logits = self.fc(last_hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "action_space = 10 # Example action space size\n",
    "model = CustomModel(config, action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# **inputs,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36mCustomDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[0;32m     34\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Apply your custom head\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mCustomActorHead.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     16\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m answer[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_action], answer[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_action :]\n\u001b[0;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mclamp_(std, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m predicted_gauss \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m predicted_gauss\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     21\u001b[0m prob \u001b[38;5;241m=\u001b[39m predicted_gauss\u001b[38;5;241m.\u001b[39mlog_prob(sample)\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\distributions\\normal.py:51\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc, scale, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, Number) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scale, Number):\n\u001b[0;32m     53\u001b[0m         batch_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize()\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\distributions\\utils.py:53\u001b[0m, in \u001b[0;36mbroadcast_all\u001b[1;34m(*values)\u001b[0m\n\u001b[0;32m     49\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     50\u001b[0m         v \u001b[38;5;28;01mif\u001b[39;00m is_tensor_like(v) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     51\u001b[0m     ]\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;241m*\u001b[39mnew_values)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=torch.tensor([[1, 1, 1, 1, 1]]),\n",
    "    attention_mask=torch.tensor([[1, 1, 1, 1, 1]]),\n",
    "    # **inputs,\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Box' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create the agent\u001b[39;00m\n\u001b[0;32m     11\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnt-v4\u001b[39m\u001b[38;5;124m'\u001b[39m, ctrl_cost_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, use_contact_forces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , verbose=1\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Retrieve the environment\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# env = model.get_env()\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20_000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:157\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef_optimizer: Optional[th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:160\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\sac\\policies.py:278\u001b[0m, in \u001b[0;36mSACPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    269\u001b[0m     {\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_critics\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_critics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m     }\n\u001b[0;32m    274\u001b[0m )\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor \u001b[38;5;241m=\u001b[39m share_features_extractor\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\sac\\policies.py:281\u001b[0m, in \u001b[0;36mSACPolicy._build\u001b[1;34m(self, lr_schedule)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr_schedule: Schedule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m    284\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs,\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\sac\\policies.py:342\u001b[0m, in \u001b[0;36mSACPolicy.make_actor\u001b[1;34m(self, features_extractor)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features_extractor: Optional[BaseFeaturesExtractor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Actor:\n\u001b[1;32m--> 342\u001b[0m     actor_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Actor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_kwargs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:114\u001b[0m, in \u001b[0;36mBaseModel._update_features_extractor\u001b[1;34m(self, net_kwargs, features_extractor)\u001b[0m\n\u001b[0;32m    111\u001b[0m net_kwargs \u001b[38;5;241m=\u001b[39m net_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# The features extractor is not shared, create a new one\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     features_extractor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m net_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(features_extractor\u001b[38;5;241m=\u001b[39mfeatures_extractor, features_dim\u001b[38;5;241m=\u001b[39mfeatures_extractor\u001b[38;5;241m.\u001b[39mfeatures_dim))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net_kwargs\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_features_extractor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseFeaturesExtractor:\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:802\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\COCUTERSASHA\\.micromamba\\envs\\ml311\\Lib\\site-packages\\transformers\\modeling_utils.py:4169\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m   4166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   4168\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[1;32m-> 4169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m   4170\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4174\u001b[0m     )\n\u001b[0;32m   4176\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[0;32m   4177\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Box' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Custom actor (pi) and value function (vf) networks\n",
    "# of two layers of size 32 each with Relu activation function\n",
    "# Note: an extra linear layer will be added on top of the pi and the vf nets, respectively\n",
    "\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "policy_kwargs = {\n",
    "    \"features_extractor_class\": DistilBertModel(config),\n",
    "    # features_extractor_kwargs=dict(config),\n",
    "    \"activation_fn\": nn.ReLU,\n",
    "    \"net_arch\": {\"pi\": [32, 32], \"qf\": [32, 32]},\n",
    "}\n",
    "# Create the agent\n",
    "env = gym.make(\n",
    "    \"Ant-v4\", ctrl_cost_weight=0.1, use_contact_forces=True, render_mode=\"human\"\n",
    ")\n",
    "model = SAC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs)  # , verbose=1\n",
    "# Retrieve the environment\n",
    "# env = model.get_env()\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=20_000)\n",
    "# Save the agent\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "# del model\n",
    "# the policy_kwargs are automatically loaded\n",
    "# model = SAC.load(\"ppo_cartpole\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.bert_config = DistilBertConfig(\n",
    "            vocab_size=10000,\n",
    "            hidden_size=16,\n",
    "            num_hidden_layers=2,\n",
    "            num_attention_heads=4,\n",
    "            intermediate_size=100,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=50,\n",
    "            type_vocab_size=2,\n",
    "            initializer_range=0.02,\n",
    "            layer_norm_eps=1e-12,\n",
    "            pad_token_id=0,\n",
    "            position_embedding_type=\"absolute\",\n",
    "            use_cache=True,\n",
    "            classifier_dropout=None,\n",
    "        )\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.distilbert_1 = DistilBertModel(self.bert_config)\n",
    "        self.distilbert_2 = DistilBertModel(self.bert_config)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, num_struct_elements=9, attention_mask=None, components_mask=None\n",
    "    ):\n",
    "        outputs_1 = self.distilbert_1(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        last_hidden_state_1 = outputs_1['last_hidden_state']\n",
    "\n",
    "        input_2 = torch.sum(last_hidden_state_1, axis=2) \n",
    "        input_2.mul_(components_mask)# summing through columns\n",
    "        input_2 = torch.sum(input_2, axis=0)\n",
    "        ones_vector = torch.ones(num_struct_elements, 1)\n",
    "        input_2 = ones_vector @ input_2.view(1, input_2.size()[0])\n",
    "        outputs_2 = self.distilbert_2(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        last_hidden_state_2 = outputs_2['last_hidden_state']\n",
    "\n",
    "        input_2 = torch.sum(last_hidden_state_2, axis=2)\n",
    "        input_2.mul_(components_mask)\n",
    "         # summing through columns\n",
    "        input_2 = torch.sum(input_2, axis=0)\n",
    "        return input_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QHead(nn.Module):\n",
    "    def __init__(self, size_input, size_action):\n",
    "        super(QHead, self).__init__()\n",
    "        self.size_input = size_input\n",
    "        self.size_action = size_action\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(self.size_input, self.size_action),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(self.size_action, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "\n",
    "\n",
    "class PiHead(nn.Module):\n",
    "    def __init__(self, size_input, size_action):\n",
    "        super(PiHead, self).__init__()\n",
    "        self.size_input = size_input\n",
    "        self.size_action = size_action\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(self.size_input, self.size_action),\n",
    "            nn.LeakyReLU(0.01),\n",
    "        )\n",
    "\n",
    "        self.last_lin = nn.Linear(1, 2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_1 = self.layer(input)\n",
    "        # ones_vector = torch.ones(self.size_action, 1)\n",
    "        # output_1 = ones_vector @ output_1.view(1, self.size_action)\n",
    "        return self.last_lin(output_1.view(self.size_action, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = v(\n",
    "    input_ids=torch.tensor([[190, 1, 80, 50, 1300], [190, 1, 80, 50, 1300]]),\n",
    "    num_struct_elements=5,\n",
    "    attention_mask=torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    components_mask=torch.tensor([[1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]),\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9605e-07,  4.1723e-07, -4.2468e-07,  1.1921e-07, -2.3842e-07],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8198, -0.7539],\n",
       "        [-0.8623, -0.8571],\n",
       "        [-0.7876, -0.6756]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = PiHead(5, 3)\n",
    "n = b(out)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
